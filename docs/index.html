<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>pylate-rs: High-Performance Sentence Embeddings with Rust & WASM</title>

    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&family=JetBrains+Mono:wght@400;500&display=swap"
        rel="stylesheet">
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/rust.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/javascript.min.js"></script>

    <style>
        /* --- Base & Typography --- */
        body {
            font-family: 'Inter', sans-serif;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            background-color: #ffffff;
            /* Cleaner white background */
            color: #374151;
            /* Default text color (gray-700) */
        }

        code,
        pre {
            font-family: 'JetBrains Mono', monospace;
        }

        .prose {
            max-width: 75ch;
            font-size: 1.1rem;
            /* Increased base text size */
            line-height: 1.7;
            text-align: justify;
            /* Justify text */
            hyphens: auto;
            /* Improve justification spacing */
        }

        .prose h2 {
            font-size: 1.5rem;
            /* Smaller H2 */
            font-weight: 600;
            margin-bottom: 1.5rem;
            padding-bottom: 0.75rem;
            border-bottom: 1px solid #e5e7eb;
            /* Lighter border */
            text-align: left;
            /* Keep headings left-aligned */
        }

        /* --- Link Styling --- */
        .prose a {
            color: #4f46e5;
            /* Indigo-600 */
            text-decoration: none;
            background-image: linear-gradient(currentColor, currentColor);
            background-position: 0% 100%;
            background-repeat: no-repeat;
            background-size: 0% 2px;
            transition: background-size .3s;
        }

        .prose a:hover {
            background-size: 100% 2px;
        }


        /* --- Code Highlighting & Containers --- */
        .hljs {
            background: #1f2937;
            border-radius: 0 0 0.5rem 0.5rem;
            padding: 1.25rem;
        }

        /* --- Styles for the Light Code Block --- */
        .code-block-container {
            position: relative;
            margin-top: 1.5rem;
        }

        .code-block-light {
            border: 1px solid #e5e7eb;
            /* border-gray-200 */
            border-radius: 0.5rem;
        }

        .code-block-light.hljs {
            background: #f7fafc;
            /* Light grey background */
            color: #2d3748;
            /* Darker grey text for better contrast */
            padding: 1.25rem;
            padding-top: 2.75rem;
            /* Reduced top padding */
        }

        /* Light theme overrides for highlight.js tokens */
        .code-block-light .hljs-comment {
            color: #718096;
        }

        /* gray-500 */
        .code-block-light .hljs-keyword {
            color: #db2777;
        }

        /* pink-600 */
        .code-block-light .hljs-string {
            color: #059669;
        }

        /* green-600 */
        .code-block-light .hljs-title.function_ {
            color: #6d28d9;
        }

        /* violet-700 */
        .code-block-light .hljs-params {
            color: #374151;
        }

        /* gray-700 */
        .code-block-light .hljs-number {
            color: #c2410c;
        }

        /* orange-600 */
        .code-block-light .hljs-meta {
            color: #4f46e5;
        }

        /* indigo-600 */
        .code-block-light .hljs-class .hljs-title {
            color: #1d4ed8;
        }

        /* blue-700 */


        .demo-container {
            border-radius: 0.5rem;
            border: 1px solid #e5e7eb;
            /* Replaced shadow with border */
            background-color: #ffffff;
        }

        /* --- Token Interaction Matrix --- */
        #token-matrix-container {
            overflow-x: auto;
            padding-bottom: 0.5rem;
        }

        .matrix {
            display: inline-grid;
            gap: 6px;
            background-color: #cbd5e1;
            border: 1px solid #cbd5e1;
            margin-top: 1.5rem;
            font-family: 'JetBrains Mono', monospace;
        }

        .matrix-cell {
            position: relative;
            background-color: #fff;
            width: 100%;
            height: 100%;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75rem;
            color: #4b5563;
            transition: background-color 0.2s ease-in-out;
            box-sizing: border-box;
        }

        /* --- Tooltip for Matrix Cells --- */
        .matrix-cell .tooltip {
            visibility: hidden;
            width: 80px;
            background-color: #1f2937;
            color: #fff;
            text-align: center;
            border-radius: 6px;
            padding: 5px 0;
            position: absolute;
            z-index: 10;
            bottom: 125%;
            left: 50%;
            margin-left: -40px;
            opacity: 0;
            transition: opacity 0.3s;
        }

        .matrix-cell:hover .tooltip {
            visibility: visible;
            opacity: 1;
        }

        /* --- Token Labels (Matrix Axes) --- */
        .token-label {
            display: flex;
            align-items: center;
            justify-content: center;
            background-color: #f3f4f6;
            height: 100%;
            width: 100%;
            padding: 0 0.5rem;
            font-size: 0.8rem;
            font-weight: 500;
            border-radius: 4px;
            white-space: nowrap;
        }

        .query-token-label {
            writing-mode: vertical-lr;
            transform: rotate(180deg);
            padding: 0.5rem 0;
        }

        /* --- Highlighted Token (Aggregated View) --- */
        .highlighted-token {
            padding: 0.2rem 0.4rem;
            border-radius: 0.375rem;
            margin: 0.1rem;
            display: inline-block;
            transition: all 0.2s ease-in-out;
            line-height: 1.5;
        }
    </style>
</head>

<body class="text-gray-700">

    <a href="https://github.com/lightonai/pylate-rs"
        class="fixed top-4 right-4 z-50 flex items-center bg-gray-800 text-white py-2 px-4 rounded-lg shadow-lg hover:bg-gray-700 transition-colors duration-300">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor"
            class="mr-2">
            <path
                d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.91 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" />
        </svg>
        <span>pylate-rs on GitHub</span>
    </a>

    <main class="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8 py-16 sm:py-24">
        <article>
            <header class="mb-16 text-center">
                <h1 class="text-4xl font-bold tracking-tight text-gray-900 mt-3">pylate-rs</h1>
                <a href="https://www.lighton.ai/" class="text-lg text-indigo-600 hover:underline">LightOn</a>
            </header>

            <div class="prose max-w-none space-y-16">
                <p style="text-indent: 40px">
                    <a href="https://github.com/lightonai/pylate">PyLate</a>, is a powerful tool for research and
                    training with ColBERT developped at <a href="https://www.lighton.ai/">LightOn</a>. It carries an
                    heavy set of
                    dependencies. That's fine for most environments and especially to train state-of-the-art information
                    retrieval models, but it can be a real headache when you just want to run inference in a live
                    application and <strong>spawn your model in milliseconds</strong>.
                    <br>
                    <br>
                    <strong>That's why we built <a href="https://github.com/lightonai/pylate-rs">pylate-rs</a></strong>.
                    The main
                    difference is that we've completely removed the PyTorch and Transformers
                    dependencies. Instead, we
                    went a different route and built it with <a href="https://github.com/huggingface/candle">Candle</a>,
                    the deep-learning crate made with Rust.
                    The goal was to create a focused, lightweight tool that does one thing well: <strong>compute ColBERT
                        embeddings</strong>.

                    <br>
                    <br>

                    If you're not familiar with ColBERT, it's a model for computing sentence embeddings. <strong>As an
                        encoder-based model, it
                        generates an embedding for each token in a sentence</strong>. The output from the final
                    Transformer layer is
                    a matrix of
                    shape <code>(embedding_dimension, num_tokens)</code>, for instance, <code>768 x num_tokens</code>.

                    <br>
                    <br>
                    <strong>A linear layer then reduces the embedding dimension</strong>, resulting in a
                    <code>128 x num_tokens</code>
                    matrix. In
                    contrast, sentence transformers don't output per-token embeddings. Instead, they aggregate all token
                    embeddings—a
                    process called pooling—using methods like mean or max pooling. This produces a single vector
                    representation for the
                    entire sentence, with a shape like <code>768 x 1</code>.
                    <br>
                    <br>
                    <a href="https://arxiv.org/abs/2004.12832">ColBERT</a> often outperforms sentence transformers
                    because it allows for
                    more <strong>fine-grained weights updates during training</strong>. If the model generates a poor
                    representation for
                    a specific
                    token, the weight adjustments can target that specific token's embedding. This is unlike standard
                    models that update
                    the entire sentence representation, even if only a small part was incorrect.
                    <br>
                    <br>
                    This per-token approach enables a powerful <strong>"late interaction"</strong> mechanism. Instead of
                    comparing two
                    fixed sentence
                    vectors, <strong>ColBERT calculates the similarity between each query token and all document
                        tokens</strong>. These
                    fine-grained scores are then aggregated to determine the final relevance score.
                    <br>
                    <br>
                    Formally, given a query <i>Q</i> with token embeddings <i>q</i><sub>i</sub> and a document <i>D</i>
                    with token embeddings <i>d</i><sub>j</sub>,
                    the <b>MaxSim</b> score is calculated by finding the maximum similarity for each query token across
                    all document
                    tokens, and then summing these maximums:
                </p>

                <div
                    style="margin: 2em 0; padding: 1.5em; background-color: #f9fafb; border: 1px solid #e5e7eb; border-radius: 0.5rem; text-align: center; overflow-x: auto;">
                    <code style="font-size: 1.2rem; color: #1f2937; background: none; padding: 0;">
                        S(Q, D) =
                        <span style="display: inline-block; text-align: center; vertical-align: middle; margin: 0 0.25em;">
                            <span style="display: block; font-size: 0.7em; line-height: 1;">|Q|</span>
                            <span style="font-size: 2em; line-height: 0.8;">&sum;</span>
                            <span style="display: block; font-size: 0.7em; line-height: 1;">i=1</span>
                        </span>
                        <span style="display: inline-block; text-align: center; vertical-align: middle; margin: 0 0.25em;">
                            <span style="display: block; font-size: 1em; line-height: 1;">max</span>
                            <span style="display: block; font-size: 0.7em; line-height: 1;">1 &le; j &le; |D|</span>
                        </span>
                        (q<sub>i</sub> &sdot; d<sub>j</sub>)
                    </code>
                </div>

                Every interactive charts below run in the browser using WebAssembly with the wasm bindings of
                pylate-rs.
                </p>
            </div>

            <div class="mt-8 demo-container p-6 sm:p-8">
                <select id="model-selector"
                    class="block w-full p-2 border-gray-300 rounded-md shadow-sm focus:border-indigo-500 focus:ring-indigo-500"></select>
                <div class="p-3 my-4 text-sm text-gray-700 bg-gray-50 rounded-lg" role="status">
                    <p id="status-text">Please select a model to begin.</p>
                </div>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-4">
                    <div>
                        <label for="query-input" class="block text-sm font-medium text-gray-700 mb-1">Query</label>
                        <textarea id="query-input" rows="3"
                            class="p-2 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500"
                            placeholder="e.g., What is the most famous museum in Paris?">What is the most famous museum in Paris?</textarea>
                    </div>
                    <div>
                        <label for="document-input" class="block text-sm font-medium text-gray-700 mb-1">Documents (one
                            per
                            line)</label>
                        <textarea id="document-input" rows="8"
                            class="p-2 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500"
                            placeholder="Enter documents here, one per line.">The Louvre Museum is the world's largest art museum and a historic monument in Paris.
The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars.
Notre-Dame de Paris, a medieval Catholic cathedral, is on the Île de la Cité.
The Musée d'Orsay is housed in the former Gare d'Orsay, a Beaux-Arts railway station.
Disneyland Paris is an entertainment resort in Chessy, 32 km east of Paris.</textarea>
                    </div>
                </div>
                <div>
                    <h3 class="text-base font-semibold text-gray-800 mb-3">Similarity Results</h3>
                    <div id="results-container" class="p-2 rounded-md border border-gray-200 min-h-[12rem]">
                        <div id="results-list" class="space-y-3 p-2"></div>
                    </div>
                </div>
            </div>

            <div class="prose max-w-none mt-16">
                <p>
                    As you can see, the MaxSim operation is a <strong>sum of maximum similarities</strong>, not an
                    average. Consequently,
                    the final score is
                    not bounded within a fixed range like <code>[0, 1]</code>. Its magnitude scales with the number of
                    tokens in the
                    query, making it
                    difficult to apply a universal similarity threshold. The score's scale depends also on the specific
                    query and document
                    context. A specific field might in average yield higher scores than another one.

                    <br>
                    <br>

                    This token-centric design also allows for visualizing the interactions between a query and a
                    document. In our
                    implementation, the token embeddings are L2-normalized. As a result, the similarity score for any
                    token pair is
                    bounded. The overall relevance score is then computed by
                    summing the maximum similarity score for each query token over all document tokens.

                </p>

            </div>

            <div class="mt-8 demo-container p-6 sm:p-8">
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-4">
                    <div>
                        <label for="matrix-query-input"
                            class="block text-sm font-medium text-gray-700 mb-1">Query</label>
                        <input type="text" id="matrix-query-input"
                            class="p-2 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500"
                            value="Where is the Sacré-Cœur located?">
                    </div>
                    <div>
                        <label for="matrix-doc-input"
                            class="block text-sm font-medium text-gray-700 mb-1">Document</label>
                        <input type="text" id="matrix-doc-input"
                            class="p-2 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500"
                            value="The Basilica of the Sacré-Cœur is located at the summit of the butte Montmartre.">
                    </div>
                </div>
                <div class="flex items-center justify-end mt-4">
                    <label for="max-sim-toggle" class="flex items-center cursor-pointer">
                        <span class="mr-3 text-sm font-medium text-gray-900">Max-Sim Only</span>
                        <div class="relative">
                            <input type="checkbox" id="max-sim-toggle" class="sr-only peer">
                            <div class="block w-11 h-6 bg-gray-200 rounded-full transition peer-checked:bg-indigo-600">
                            </div>
                            <div
                                class="dot absolute left-1 top-1 bg-white w-4 h-4 rounded-full transition-transform peer-checked:translate-x-full">
                            </div>
                        </div>
                    </label>
                </div>
                <div id="token-matrix-container" class="pt-2">
                    <p id="matrix-status-text" class="p-3 text-sm text-center text-gray-600 bg-gray-50 rounded-lg"
                        role="status">
                        Waiting for model to load...</p>
                </div>
            </div>

            <div class="prose max-w-none mt-16">

                <p>
                    By summing the similarity scores of the document tokens that contribute to the MaxSim calculation,
                    we can visualize the
                    weight of each token in the final score. These are the specific document tokens that had the highest
                    similarity for each
                    corresponding query token. The visualization is freely inspired from <a
                        href="https://colbert.aiserv.cloud">the excellent Jo Kristian Bergum demo</a>.
                    <br>

                    However, it's crucial to remember that <strong>the token embeddings are contextualized</strong>.
                    This means a token
                    can indirectly
                    influence the score even if it is not highlighted as a top-scoring match. It does so by altering the
                    embeddings of its
                    neighboring tokens, which may be the ones that are directly measured. The final score is the sum of
                    the scores from the
                    highlighted tokens, but their values are shaped by the entire context.
                </p>
            </div>

            <div class="mt-8 demo-container p-6 sm:p-8">
                <div id="token-importance-container"
                    class="p-4 bg-gray-50 rounded-lg min-h-[6rem] text-base leading-relaxed">
                    <p class="text-gray-500 text-center text-sm">Enter a query and document above to see the
                        visualization.</p>
                </div>
            </div>

            <div class="prose max-w-none mt-16">

                <p>
                    It may seem <strong>paradoxical</strong> to praise ColBERT for its <strong>token-level
                        granularity</strong> only to
                    then find ways to reduce the number of token embeddings we use. In reality, this reflects a
                    <strong>practical
                        trade-off</strong> between <strong>computational cost</strong> and <strong>representational
                        detail</strong>.

                    <br>
                    <br>

                    <code>pylate-rs</code> implements a token reduction strategy following the <a
                        href="https://www.answer.ai/posts/colbert-pooling.html"> article
                        from Benjamin Clavié</a>. The core idea is to find a <strong>balance between using all tokens
                        and using
                        only the most
                        salient ones</strong>.

                    <br>
                    <br>

                    Use the slider below to adjust this pooling factor. You will see how it simplifies the document
                    representation and
                    affects the final similarity score, illustrating the direct trade-off between
                    <strong>performance</strong> and
                    <strong>accuracy</strong>.
                </p>

            </div>

            <div class="mt-8 demo-container p-6 sm:p-8">
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                    <div>
                        <label for="pooling-doc-input" class="block text-sm font-medium text-gray-700 mb-1">Long
                            Document</label>
                        <textarea id="pooling-doc-input" rows="8"
                            class="p-2 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500">The Latin Quarter of Paris is located on the left bank of the Seine, surrounding the Sorbonne. Known for its student life, lively atmosphere, and bistros, it is the home to a number of higher education establishments besides the university itself. The area gets its name from the Latin language, which was once widely spoken in and around the University since Latin was the language of learning in the Middle Ages.</textarea>
                    </div>
                    <div>
                        <label for="pooling-query-input"
                            class="block text-sm font-medium text-gray-700 mb-1">Query</label>
                        <input id="pooling-query-input" type="text"
                            class="p-2 block w-full rounded-md border-gray-300 shadow-sm focus:border-indigo-500 focus:ring-indigo-500"
                            value="Why is it called the Latin Quarter?">

                        <div class="mt-6">
                            <label for="pool-factor-slider" class="block text-sm font-medium text-gray-700">Pool
                                Factor:
                                <span id="pool-factor-value" class="font-bold text-indigo-600">1</span></label>
                            <input id="pool-factor-slider" type="range" min="1" max="5" value="1"
                                class="w-full h-2 bg-gray-200 rounded-lg appearance-none cursor-pointer accent-indigo-600">
                        </div>
                    </div>
                </div>
                <div id="pooling-score-output"
                    class="mt-6 grid grid-cols-1 md:grid-cols-2 gap-4 text-center p-4 bg-gray-50 rounded-lg">
                    <div>
                        <p class="text-sm font-medium text-gray-500">Original Score</p>
                        <p id="original-score" class="text-2xl font-bold text-gray-800">-</p>
                    </div>
                    <div>
                        <p class="text-sm font-medium text-gray-500">Pooled Score</p>
                        <p id="pooled-score" class="text-2xl font-bold text-indigo-600">-</p>
                    </div>
                </div>
                <div id="pooling-status" class="mt-4 p-3 text-sm text-gray-700 bg-gray-50 rounded-lg" role="status">
                    <p>Waiting for model to load...</p>
                </div>
                <div class="mt-6 grid grid-cols-1 md:grid-cols-2 gap-6">
                    <div>
                        <h3 class="text-base font-semibold text-gray-800 mb-3">Original Embeddings</h3>
                        <p class="text-sm text-gray-500 mb-3">Tokens: <span id="original-token-count">0</span>
                        </p>
                        <div id="original-embeddings-vis"
                            class="p-3 bg-white border rounded-md min-h-[10rem] flex flex-wrap gap-1 content-start">
                        </div>
                    </div>
                    <div>
                        <h3 class="text-base font-semibold text-gray-800 mb-3">Pooled Embeddings</h3>
                        <p class="text-sm text-gray-500 mb-3">Tokens: <span id="pooled-token-count">0</span></p>
                        <div id="pooled-embeddings-vis"
                            class="p-3 bg-white border rounded-md min-h-[10rem] flex flex-wrap gap-1 content-start">
                        </div>
                    </div>
                </div>

            </div>

            <div class="prose max-w-none mt-16">

                <p>
                    At LightOn, we are developing Generative Models, Encoders,
                    ColBERT models and <a
                        href="https://www.lighton.ai/lighton-blogs/lighton-integrates-visual-rag-into-its-platform">state-of-the-art
                        RAG pipelines</a>. We released PyLate, an optimized solution for training ColBERT
                    models on hardware ranging from a single CPU to a multi-GPU node.

                    <br>
                    <br>

                    In partnership with AnswerAI, LightOn released <a
                        href="https://arxiv.org/abs/2412.13663">ModernBERT</a>, a new encoder that we fine-tuned to
                    create <a
                        href="https://www.lighton.ai/lighton-blogs/lighton-releases-gte-moderncolbert-first-state-of-the-art-late-interaction-model-trained-on-pylate">GTE-ModernColBERT</a>.

                    <br>
                    <br>

                    LightOn also released <a
                        href="https://www.lighton.ai/lighton-blogs/lighton-releases-reason-colbert">Reason-ModernColBERT</a>,
                    which achieves state-of-the-art results on
                    the <a href="https://huggingface.co/datasets/xlangai/BRIGHT">BRIGHT</a>
                    benchmark. <strong>Both GTE-ModernColBERT and Reason-ModernColBERT models were trained with PyLate
                        and are compatible with <code>pylate-rs</code></strong>.

                    <br>
                    <br>

                    You can find all compatible models on the Hugging Face Hub under the <a
                        href="https://huggingface.co/models?other=PyLate">PyLate tag</a>.

                    <br>
                    <br>

                    Finally, we released <a href="https://github.com/lightonai/fast-plaid">fast-plaid</a>, a Rust
                    implementation of the <a href="https://arxiv.org/abs/2205.09707">PLAID</a> algorithm for
                    <strong>efficient nearest-neighbor search</strong>. Paired with <code>pylate-rs</code>, fast-plaid
                    offers a
                    lightweight solution for running ColBERT as a retriever in Python. Currently, fast-plaid is
                    immutable, meaning the
                    index must be rebuilt to add new documents. For use cases requiring mutable indexes, we recommend
                    exploring
                    solutions like <a href="https://weaviate.io/blog/muvera">Weaviate's
                        implementation</a> of <a
                        href="https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/">MUVERA</a>.
                    We plan to add mutability and filtering to fast-plaid in the future. Any contribution is welcome!

                    <br>
                    <br>

                    Here is a sample code for running ColBERT with pylate-rs and fast-plaid. <strong>This is the fastest
                        way
                        to create a multi-vectors index</strong> apart from calling pylate-rs from rust ⚡️. It's
                    compatible with CUDA and CPUs and batch-oriented.
                </p>
                </p>

                <div class="code-block-container">
                    <button
                        class="copy-button absolute top-3 right-3 bg-gray-200 hover:bg-gray-300 text-gray-800 font-sans text-xs font-semibold py-1 px-3 rounded-md flex items-center transition-colors duration-200">
                        <span class="copy-icon mr-2">
                            <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24"
                                fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                                stroke-linejoin="round" class="lucide lucide-clipboard">
                                <rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect>
                                <path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2">
                                </path>
                            </svg>
                        </span>
                        <span class="copy-text">Copy</span>
                    </button>
                    <pre><code class="language-python code-block-light">
import torch
from fast_plaid import search
from pylate_rs import models

model = models.ColBERT(
    model_name_or_path="lightonai/GTE-ModernColBERT-v1",
    device="cpu", # mps or cuda
)


documents = [
    "1st Arrondissement: Louvre, Tuileries Garden, Palais Royal, historic, tourist.",
    "2nd Arrondissement: Bourse, financial, covered passages, Sentier, business.",
    "3rd Arrondissement: Marais, Musée Picasso, galleries, trendy, historic.",
    "4th Arrondissement: Notre-Dame, Marais, Hôtel de Ville, LGBTQ+.",
    "5th Arrondissement: Latin Quarter, Sorbonne, Panthéon, student, intellectual.",
    "6th Arrondissement: Saint-Germain-des-Prés, Luxembourg Gardens, chic, artistic, cafés.",
    "7th Arrondissement: Eiffel Tower, Musée d'Orsay, Les Invalides, affluent, prestigious.",
    "8th Arrondissement: Champs-Élysées, Arc de Triomphe, luxury, shopping, Élysée.",
    "9th Arrondissement: Palais Garnier, department stores, shopping, theaters.",
    "10th Arrondissement: Gare du Nord, Gare de l'Est, Canal Saint-Martin.",
    "11th Arrondissement: Bastille, nightlife, Oberkampf, revolutionary, hip.",
    "12th Arrondissement: Bois de Vincennes, Opéra Bastille, Bercy, residential.",
    "13th Arrondissement: Chinatown, Bibliothèque Nationale, modern, diverse, street-art.",
    "14th Arrondissement: Montparnasse, Catacombs, residential, artistic, quiet.",
    "15th Arrondissement: Residential, family, populous, Parc André Citroën.",
    "16th Arrondissement: Trocadéro, Bois de Boulogne, affluent, elegant, embassies.",
    "17th Arrondissement: Diverse, Palais des Congrès, residential, Batignolles.",
    "18th Arrondissement: Montmartre, Sacré-Cœur, Moulin Rouge, artistic, historic.",
    "19th Arrondissement: Parc de la Villette, Cité des Sciences, canals, diverse.",
    "20th Arrondissement: Père Lachaise, Belleville, cosmopolitan, artistic, historic.",
]

# Encoding documents
documents_embeddings = model.encode(
    sentences=documents,
    is_query=False,
    pool_factor=2, # Let's divide the number of embeddings by 2.
)

# Creating the FastPlaid index
fast_plaid = search.FastPlaid(index="index")


fast_plaid.create(
    documents_embeddings=[torch.tensor(embedding) for embedding in documents_embeddings]
)
                        </code></pre>


                    <div class="prose max-w-none mt-16">

                        <p>
                            We can then load the existing index and search for the most relevant documents:
                        </p>

                        <div class="code-block-container">
                            <button
                                class="copy-button absolute top-3 right-3 bg-gray-200 hover:bg-gray-300 text-gray-800 font-sans text-xs font-semibold py-1 px-3 rounded-md flex items-center transition-colors duration-200">
                                <span class="copy-icon mr-2">
                                    <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24"
                                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                                        stroke-linejoin="round" class="lucide lucide-clipboard">
                                        <rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect>
                                        <path
                                            d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2">
                                        </path>
                                    </svg>
                                </span>
                                <span class="copy-text">Copy</span>
                            </button>
                            <pre><code class="language-python code-block-light">
import torch
from fast_plaid import search
from pylate_rs import models

fast_plaid = search.FastPlaid(index="index")

queries = [
    "arrondissement with the Eiffel Tower and Musée d'Orsay",
    "Latin Quarter and Sorbonne University",
    "arrondissement with Sacré-Cœur and Moulin Rouge",
    "arrondissement with the Louvre and Tuileries Garden",
    "arrondissement with Notre-Dame Cathedral and the Marais",
]

queries_embeddings = model.encode(
    sentences=queries,
    is_query=True,
)

scores = fast_plaid.search(
    queries_embeddings=torch.tensor(queries_embeddings),
    top_k=3,
)

print(scores)
                                        </code></pre>
                        </div>
        </article>
        <div class="prose max-w-none mt-16">
            <p class="text-left" style="font-size: 1rem;">
                <a href="https://github.com/raphaelsty">Raphael Sourty</a>, AI @
                LightOn
                <span class="text-gray-500"> - July 3, 2025</span>
            </p>
        </div>
        <div class="prose max-w-none mt-4">
            <p class="text-left" style="font-size: 1rem;">
                PyLate is being built with my amazing co-maintainer, <a href="https://github.com/nohtow">Antoine
                    Chaffin</a>.
            </p>
        </div>
    </main>

    <script type="module">
        import init, {
            ColBERT,
            hierarchical_pooling
        } from "./pkg/pylate_rs.js";

        /**
         * ----------------------------------------------------------------
         * Application Constants
         * ----------------------------------------------------------------
         */
        const CONSTANTS = {
            DEBOUNCE_DELAY_INPUT: 400,
            DEBOUNCE_DELAY_MATRIX: 500,
            DEBOUNCE_DELAY_SLIDER: 50,
            DEFAULT_MODEL: "answerai-colbert-small-v1",
            AVAILABLE_MODELS: [
                "answerai-colbert-small-v1",
                "lightonai/GTE-ModernColBERT-v1",
                "lightonai/colbertv2.0",
                "lightonai/Reason-ModernColBERT",
            ],
            REQUIRED_FILES: [
                'tokenizer.json', 'model.safetensors', 'config.json',
                'config_sentence_transformers.json', '1_Dense/model.safetensors', '1_Dense/config.json',
                "special_tokens_map.json"
            ],
            CELL_SIZE: "3.5rem"
        };

        /**
         * ----------------------------------------------------------------
         * Utility Functions
         * ----------------------------------------------------------------
         */
        const Utils = {
            /**
             * Creates a debounced function that delays invoking `func` until after `wait` milliseconds.
             */
            debounce(func, wait) {
                let timeout;
                return function (...args) {
                    const context = this;
                    clearTimeout(timeout);
                    timeout = setTimeout(() => func.apply(context, args), wait);
                };
            },

            /**
             * Interpolates a color from a viridis-like color scale.
             * @param {number} intensity - A value between 0 and 1.
             * @returns {string} An RGB color string.
             */
            getColorForIntensity(intensity) {
                const colorStops = [{
                    p: 0.0,
                    color: [68, 1, 84]
                }, {
                    p: 0.25,
                    color: [59, 82, 139]
                }, {
                    p: 0.5,
                    color: [33, 145, 140]
                }, {
                    p: 0.75,
                    color: [94, 201, 98]
                }, {
                    p: 1.0,
                    color: [253, 231, 37]
                }];

                if (intensity <= 0) return `rgb(${colorStops[0].color.join(',')})`;
                if (intensity >= 1) return `rgb(${colorStops[colorStops.length - 1].color.join(',')})`;

                let lower, upper;
                for (let i = 1; i < colorStops.length; i++) {
                    if (intensity < colorStops[i].p) {
                        lower = colorStops[i - 1];
                        upper = colorStops[i];
                        break;
                    }
                }
                const factor = (intensity - lower.p) / (upper.p - lower.p);
                const r = Math.round(lower.color[0] + factor * (upper.color[0] - lower.color[0]));
                const g = Math.round(lower.color[1] + factor * (upper.color[1] - lower.color[1]));
                const b = Math.round(lower.color[2] + factor * (upper.color[2] - lower.color[2]));
                return `rgb(${r},${g},${b})`;
            },
        };

        /**
         * ----------------------------------------------------------------
         * Main Application Object
         * ----------------------------------------------------------------
         */
        const App = {
            // --- Application State ---
            state: {
                colbertModel: null,
                currentModelName: CONSTANTS.DEFAULT_MODEL,
                showMaxSimOnly: false,
                isLoading: false,
            },

            // --- UI Element Selectors ---
            ui: {
                // Model & Status
                modelSelector: document.getElementById('model-selector'),
                statusText: document.getElementById('status-text'),
                // Similarity Demo
                queryInput: document.getElementById('query-input'),
                documentInput: document.getElementById('document-input'),
                resultsList: document.getElementById('results-list'),
                // Matrix Demo
                matrixContainer: document.getElementById('token-matrix-container'),
                matrixQueryInput: document.getElementById('matrix-query-input'),
                matrixDocInput: document.getElementById('matrix-doc-input'),
                maxSimToggle: document.getElementById('max-sim-toggle'),
                matrixStatusText: document.getElementById('matrix-status-text'),
                // Token Importance Demo
                tokenImportanceContainer: document.getElementById('token-importance-container'),
                // Pooling Demo
                poolingDocInput: document.getElementById('pooling-doc-input'),
                poolingQueryInput: document.getElementById('pooling-query-input'),
                poolFactorSlider: document.getElementById('pool-factor-slider'),
                poolFactorValue: document.getElementById('pool-factor-value'),
                originalTokenCount: document.getElementById('original-token-count'),
                pooledTokenCount: document.getElementById('pooled-token-count'),
                originalEmbeddingsVis: document.getElementById('original-embeddings-vis'),
                pooledEmbeddingsVis: document.getElementById('pooled-embeddings-vis'),
                poolingStatus: document.getElementById('pooling-status'),
                originalScore: document.getElementById('original-score'),
                pooledScore: document.getElementById('pooled-score'),
            },

            /**
             * Initializes the application, sets up event listeners, and loads the initial model.
             */
            async init() {
                this.initEventListeners();
                this.populateModelSelector();
                this.initCopyButtons();
                hljs.highlightAll();

                try {
                    await init();
                    this.loadModel(this.state.currentModelName);
                } catch (e) {
                    console.error("Fatal: Failed to initialize WASM module.", e);
                    this.updateAllStatuses(`Error: Could not initialize the application.`, 'error');
                }
            },

            /**
             * Registers all event listeners for UI elements.
             */
            initEventListeners() {
                const debouncedSimilarity = Utils.debounce(this.handleSimilarityDemo, CONSTANTS.DEBOUNCE_DELAY_INPUT);
                const debouncedMatrix = Utils.debounce(this.handleMatrixDemo, CONSTANTS.DEBOUNCE_DELAY_MATRIX);
                const debouncedPooling = Utils.debounce(this.handlePoolingDemo, CONSTANTS.DEBOUNCE_DELAY_INPUT);
                const debouncedSlider = Utils.debounce(this.handlePoolingDemo, CONSTANTS.DEBOUNCE_DELAY_SLIDER);

                this.ui.modelSelector.addEventListener('change', e => this.loadModel(e.target.value));
                this.ui.queryInput.addEventListener('input', debouncedSimilarity.bind(this));
                this.ui.documentInput.addEventListener('input', debouncedSimilarity.bind(this));
                this.ui.matrixQueryInput.addEventListener('input', debouncedMatrix.bind(this));
                this.ui.matrixDocInput.addEventListener('input', debouncedMatrix.bind(this));
                this.ui.maxSimToggle.addEventListener('change', e => {
                    this.state.showMaxSimOnly = e.target.checked;
                    this.handleMatrixDemo();
                });
                this.ui.poolingDocInput.addEventListener('input', debouncedPooling.bind(this));
                this.ui.poolingQueryInput.addEventListener('input', debouncedPooling.bind(this));
                this.ui.poolFactorSlider.addEventListener('input', () => {
                    this.ui.poolFactorValue.textContent = this.ui.poolFactorSlider.value;
                    debouncedSlider.call(this);
                });
            },

            /**
             * Initializes all copy-to-clipboard buttons for code blocks.
             */
            initCopyButtons() {
                const copyButtons = document.querySelectorAll('.copy-button');
                if (!copyButtons.length) return;

                const successIcon = `<svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-check"><polyline points="20 6 9 17 4 12"></polyline></svg>`;

                copyButtons.forEach(button => {
                    const iconSpan = button.querySelector('.copy-icon');
                    const textSpan = button.querySelector('.copy-text');
                    const originalIcon = iconSpan.innerHTML;

                    button.addEventListener('click', () => {
                        const container = button.closest('.code-block-container');
                        const codeBlock = container ? container.querySelector('pre code') : null;

                        if (codeBlock) {
                            navigator.clipboard.writeText(codeBlock.innerText).then(() => {
                                textSpan.textContent = 'Copied!';
                                iconSpan.innerHTML = successIcon;

                                setTimeout(() => {
                                    textSpan.textContent = 'Copy';
                                    iconSpan.innerHTML = originalIcon;
                                }, 2000);
                            }).catch(err => {
                                console.error('Failed to copy text: ', err);
                                textSpan.textContent = 'Error';
                                setTimeout(() => {
                                    textSpan.textContent = 'Copy';
                                }, 2000);
                            });
                        }
                    });
                });
            },

            /**
             * Populates the model selector dropdown with available models.
             */
            populateModelSelector() {
                CONSTANTS.AVAILABLE_MODELS.forEach(modelName => {
                    const option = document.createElement('option');
                    option.value = modelName;
                    option.textContent = modelName;
                    this.ui.modelSelector.appendChild(option);
                });
                this.ui.modelSelector.value = this.state.currentModelName;
            },

            /**
             * Updates status messages across all relevant UI components.
             * @param {string} message - The HTML message to display.
             * @param {'info'|'error'|'loading'|'success'} type - The type of message.
             */
            updateAllStatuses(message, type = 'info') {
                const isError = type === 'error';
                const baseClass = isError ? 'text-red-600' : 'text-gray-700';

                this.ui.statusText.innerHTML = message;
                this.ui.statusText.parentElement.className = `p-3 my-4 text-sm rounded-lg ${isError ? 'bg-red-50 text-red-700' : 'bg-gray-50 text-gray-700'}`;

                this.ui.matrixContainer.innerHTML = `<p class="${baseClass} text-center p-4">${message}</p>`;
                this.ui.poolingStatus.innerHTML = `<p class="${baseClass}">${message}</p>`;
                this.ui.tokenImportanceContainer.innerHTML = `<p class="${baseClass} text-center text-sm">${message}</p>`;
            },

            /**
             * Loads a ColBERT model, fetching files either locally or from Hugging Face.
             * @param {string} modelRepo - The repository name of the model.
             */
            async loadModel(modelRepo) {
                if (this.state.isLoading) return;
                this.state.isLoading = true;
                this.state.colbertModel = null;
                this.state.currentModelName = modelRepo;
                this.updateAllStatuses(`⏳ Loading ${modelRepo}...`, 'loading');

                const fetchAllFiles = async (basePath) => {
                    const responses = await Promise.all(
                        CONSTANTS.REQUIRED_FILES.map(file => fetch(`${basePath}/${file}`))
                    );
                    for (const response of responses) {
                        if (!response.ok) throw new Error(`File not found: ${response.url}`);
                    }
                    return Promise.all(responses.map(res => res.arrayBuffer().then(b => new Uint8Array(b))));
                };

                try {
                    let modelFiles;
                    try {
                        modelFiles = await fetchAllFiles(`models/${modelRepo}`);
                    } catch (e) {
                        console.warn(`Local model not found, falling back to Hugging Face Hub.`, e);
                        this.updateAllStatuses(`⏳ Downloading ${modelRepo} from Hugging Face Hub...`, 'loading');
                        modelFiles = await fetchAllFiles(`https://huggingface.co/${modelRepo}/resolve/main`);
                    }

                    this.updateAllStatuses(`Initializing ${modelRepo}...`, 'loading');
                    const [tokenizer, model, config, stConfig, dense, denseConfig, tokensConfig] = modelFiles;
                    this.state.colbertModel = new ColBERT(model, dense, tokenizer, config, stConfig, denseConfig, tokensConfig, 32);

                    this.updateAllStatuses(`✅ ${modelRepo} loaded successfully.`, 'success');
                    this.runAllDemos();
                } catch (error) {
                    console.error("Model Loading Error:", error);
                    const msg = `Error: Could not load model ${modelRepo}.`;
                    this.updateAllStatuses(msg, 'error');
                } finally {
                    this.state.isLoading = false;
                }
            },

            /**
             * Runs all interactive demos after a model is loaded.
             */
            runAllDemos() {
                this.handleSimilarityDemo();
                this.handleMatrixDemo();
                this.handlePoolingDemo();
            },

            /**
             * SIMILARITY DEMO: Calculates and displays similarity scores.
             */
            handleSimilarityDemo() {
                if (!this.state.colbertModel) return;
                const query = this.ui.queryInput.value;
                const documents = this.ui.documentInput.value.split('\n').filter(doc => doc.trim());
                this.ui.resultsList.innerHTML = '';

                if (!query.trim() || documents.length === 0) {
                    this.ui.resultsList.innerHTML = `<p class="text-gray-500 p-2">Enter a query and at least one document.</p>`;
                    return;
                }

                try {
                    const {
                        data
                    } = this.state.colbertModel.similarity({
                        queries: [query],
                        documents
                    });
                    const scores = data[0];
                    const results = documents.map((doc, i) => ({
                        text: doc,
                        score: scores[i]
                    }))
                        .sort((a, b) => b.score - a.score);

                    const maxScore = results.length > 0 && results[0].score > 0 ? results[0].score : 1;
                    const fragment = document.createDocumentFragment();
                    results.forEach(result => {
                        const scorePercentage = (result.score / maxScore) * 100;
                        const resultEl = document.createElement('div');
                        resultEl.className = 'bg-white p-3 rounded-md border border-gray-200';
                        resultEl.innerHTML = `
                            <p class="text-gray-800 mb-2 text-base">${result.text}</p>
                            <div class="flex items-center gap-2">
                                <span class="text-xs font-semibold text-indigo-700 w-24">Score: ${result.score.toFixed(3)}</span>
                                <div class="w-full bg-indigo-100 rounded-full h-2">
                                    <div class="bg-indigo-500 h-2 rounded-full" style="width: ${scorePercentage}%"></div>
                                </div>
                            </div>`;
                        fragment.appendChild(resultEl);
                    });
                    this.ui.resultsList.appendChild(fragment);
                } catch (error) {
                    this.ui.resultsList.innerHTML = `<p class="text-red-600">Error during calculation. See console.</p>`;
                    console.error("Similarity Calculation Error:", error);
                }
            },

            /**
             * MATRIX DEMO: Orchestrates fetching data and rendering the interaction matrix and token importance.
             */
            handleMatrixDemo() {
                if (!this.state.colbertModel) return;
                const query = this.ui.matrixQueryInput.value;
                const doc = this.ui.matrixDocInput.value;

                if (!query.trim() || !doc.trim()) {
                    this.ui.matrixContainer.innerHTML = `<p class="text-gray-500 text-center p-4">Please provide a query and a document.</p>`;
                    this.ui.tokenImportanceContainer.innerHTML = `<p class="text-gray-500 text-center text-sm">Enter a query and document to begin.</p>`;
                    return;
                }

                try {
                    const rawResult = this.state.colbertModel.raw_similarity_matrix({
                        queries: [query],
                        documents: [doc]
                    });
                    const {
                        matrix,
                        queryTokens,
                        docTokens,
                        maxSimIndicesByQuery,
                        maxSimDocIndices
                    } = this.processMatrixData(rawResult);

                    if (queryTokens.length === 0 || docTokens.length === 0) {
                        this.ui.matrixContainer.innerHTML = `<p class="text-gray-500 text-center p-4">No valid tokens to display.</p>`;
                        return;
                    }

                    this.renderMatrix(matrix, queryTokens, docTokens, maxSimIndicesByQuery);
                    this.renderTokenImportance(matrix, docTokens, maxSimDocIndices);
                } catch (error) {
                    this.ui.matrixContainer.innerHTML = `<p class="text-red-600 text-center p-4">Failed to generate visualizations.</p>`;
                    console.error("Visualization Error:", error);
                }
            },

            /**
             * Processes raw similarity data, filtering out MASK tokens and pre-calculating values for rendering.
             */
            processMatrixData(rawResult) {
                const [originalMatrix] = rawResult.similarity_matrix[0];
                const [originalQueryTokens] = rawResult.query_tokens;
                const [originalDocTokens] = rawResult.document_tokens;

                const filterTokens = (tokens) => {
                    const indices = [];
                    const filtered = tokens.filter((token, i) => {
                        const keep = token !== '[MASK]';
                        if (keep) indices.push(i);
                        return keep;
                    });
                    return {
                        indices,
                        filtered
                    };
                };

                const {
                    indices: queryIndices,
                    filtered: queryTokens
                } = filterTokens(originalQueryTokens);
                const {
                    indices: docIndices,
                    filtered: docTokens
                } = filterTokens(originalDocTokens);

                const matrix = queryIndices.map(qIdx => docIndices.map(dIdx => originalMatrix[qIdx][dIdx]));

                const maxSimIndicesByQuery = [];
                const maxSimDocIndices = new Set();
                matrix.forEach(row => {
                    let maxScore = -Infinity,
                        maxIdx = -1;
                    row.forEach((score, docIdx) => {
                        if (score > maxScore) {
                            maxScore = score;
                            maxIdx = docIdx;
                        }
                    });
                    maxSimIndicesByQuery.push(maxIdx);
                    if (maxIdx !== -1) maxSimDocIndices.add(maxIdx);
                });

                return {
                    matrix,
                    queryTokens,
                    docTokens,
                    maxSimIndicesByQuery,
                    maxSimDocIndices
                };
            },

            /**
             * Renders the token-to-token similarity matrix grid.
             */
            renderMatrix(matrix, queryTokens, docTokens, maxSimIndicesByQuery) {
                const flatScores = matrix.flat();
                const minScore = Math.min(...flatScores),
                    maxScore = Math.max(...flatScores);

                const grid = document.createElement('div');
                grid.className = 'matrix';
                grid.style.gridTemplateColumns = `auto repeat(${docTokens.length}, minmax(${CONSTANTS.CELL_SIZE}, auto))`;
                grid.style.gridTemplateRows = `auto repeat(${queryTokens.length}, ${CONSTANTS.CELL_SIZE})`;

                // Top-left empty cell
                grid.appendChild(document.createElement('div'));

                // Document token headers
                docTokens.forEach(token => {
                    const label = document.createElement('div');
                    label.className = 'token-label';
                    label.textContent = token;
                    grid.appendChild(label);
                });

                // Query tokens and score cells
                queryTokens.forEach((queryToken, i) => {
                    const queryLabel = document.createElement('div');
                    queryLabel.className = 'token-label query-token-label';
                    queryLabel.textContent = queryToken;
                    grid.appendChild(queryLabel);

                    const maxScoreDocIndex = maxSimIndicesByQuery[i];
                    docTokens.forEach((_, j) => {
                        const cell = document.createElement('div');
                        cell.className = 'matrix-cell';
                        const score = matrix[i][j];
                        const intensity = (maxScore > minScore) ? (score - minScore) / (maxScore - minScore) : 0;
                        const color = Utils.getColorForIntensity(intensity);

                        if (this.state.showMaxSimOnly) {
                            cell.style.backgroundColor = (j === maxScoreDocIndex) ? color : '#f9fafb';
                            if (j === maxScoreDocIndex) cell.style.border = '2px solid #111827';
                        } else {
                            cell.style.backgroundColor = color;
                        }

                        cell.innerHTML = `<span class="tooltip">${score.toFixed(2)}</span>`;
                        grid.appendChild(cell);
                    });
                });

                this.ui.matrixContainer.innerHTML = '';
                this.ui.matrixContainer.appendChild(grid);
            },

            /**
             * Renders the aggregated token importance visualization.
             */
            renderTokenImportance(matrix, docTokens, maxSimDocIndices) {
                const columnSums = new Array(docTokens.length).fill(0);
                matrix.forEach(row => row.forEach((score, j) => columnSums[j] += score));

                const minSum = Math.min(...columnSums),
                    maxSum = Math.max(...columnSums);
                const range = maxSum - minSum;

                this.ui.tokenImportanceContainer.innerHTML = '';
                const fragment = document.createDocumentFragment();
                docTokens.forEach((token, i) => {
                    const tokenSpan = document.createElement('span');
                    if (maxSimDocIndices.has(i)) {
                        const normScore = range > 0 ? (columnSums[i] - minSum) / range : 0;
                        tokenSpan.className = 'highlighted-token';
                        tokenSpan.style.backgroundColor = Utils.getColorForIntensity(normScore);
                        tokenSpan.style.color = normScore > 0.6 ? '#f8fafc' : '#1f2937';
                        tokenSpan.textContent = token;
                    } else {
                        tokenSpan.textContent = token + ' '; // Add space for non-highlighted tokens
                        tokenSpan.style.color = '#6b7280';
                    }
                    fragment.appendChild(tokenSpan);
                });
                this.ui.tokenImportanceContainer.appendChild(fragment);
            },

            /**
             * POOLING DEMO: Calculates and displays scores for original vs. pooled embeddings.
             */
            handlePoolingDemo() {
                if (!this.state.colbertModel) return;
                this.ui.poolingStatus.textContent = '⚙️ Processing...';

                const docText = this.ui.poolingDocInput.value;
                const queryText = this.ui.poolingQueryInput.value;
                const poolFactor = parseInt(this.ui.poolFactorSlider.value, 10);

                this.ui.originalScore.textContent = '-';
                this.ui.pooledScore.textContent = '-';

                if (!docText.trim()) {
                    this.ui.poolingStatus.textContent = 'Please enter a document.';
                    this.renderEmbeddingBlocks(this.ui.originalEmbeddingsVis, 0, this.ui.originalTokenCount);
                    this.renderEmbeddingBlocks(this.ui.pooledEmbeddingsVis, 0, this.ui.pooledTokenCount);
                    return;
                }

                try {
                    const originalDocResult = this.state.colbertModel.encode({
                        sentences: [docText]
                    }, false);
                    const originalDocEmbeddings = originalDocResult.embeddings[0] || [];
                    this.renderEmbeddingBlocks(this.ui.originalEmbeddingsVis, originalDocEmbeddings.length, this.ui.originalTokenCount);

                    const pooledResult = hierarchical_pooling({
                        embeddings: [originalDocEmbeddings],
                        pool_factor: poolFactor
                    });
                    const pooledDocEmbeddings = pooledResult.embeddings[0] || [];
                    this.renderEmbeddingBlocks(this.ui.pooledEmbeddingsVis, pooledDocEmbeddings.length, this.ui.pooledTokenCount);

                    if (queryText.trim()) {
                        const queryResult = this.state.colbertModel.encode({
                            sentences: [queryText]
                        }, false);
                        const queryEmbeddings = queryResult.embeddings[0] || [];

                        const calculateScore = (qVecs, dVecs) => {
                            if (!qVecs?.length || !dVecs?.length) return 0;
                            let totalScore = 0;
                            for (const qVec of qVecs) {
                                let maxSim = -Infinity;
                                for (const dVec of dVecs) {
                                    maxSim = Math.max(maxSim, qVec.reduce((sum, val, i) => sum + val * dVec[i], 0));
                                }
                                if (maxSim > -Infinity) totalScore += maxSim;
                            }
                            return totalScore;
                        };

                        this.ui.originalScore.textContent = calculateScore(queryEmbeddings, originalDocEmbeddings).toFixed(4);
                        this.ui.pooledScore.textContent = calculateScore(queryEmbeddings, pooledDocEmbeddings).toFixed(4);
                        this.ui.poolingStatus.textContent = '✅ Done!';
                    } else {
                        this.ui.poolingStatus.textContent = 'Enter a query to see the scores.';
                    }
                } catch (e) {
                    this.ui.poolingStatus.textContent = 'An error occurred. See console.';
                    console.error("Pooling Demo Error:", e);
                }
            },

            /**
             * Renders visual blocks representing token embeddings.
             */
            renderEmbeddingBlocks(container, count, countElement) {
                container.innerHTML = '';
                countElement.textContent = count;
                if (count === 0) return;
                const fragment = document.createDocumentFragment();
                for (let i = 0; i < count; i++) {
                    const block = document.createElement('div');
                    block.className = 'w-3 h-3 bg-indigo-400 rounded-sm';
                    fragment.appendChild(block);
                }
                container.appendChild(fragment);
            },
        };

        // --- Application Entry Point ---
        document.addEventListener('DOMContentLoaded', () => App.init());
    </script>

</body>

</html>